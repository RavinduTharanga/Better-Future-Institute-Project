# -*- coding: utf-8 -*-
"""Checkpoint_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hvyDOhpvDsgqcE7arQVBSPIg0h4aS0vH
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score

# Selecting a subset of features for simplicity
# Assuming 'Status' as the target variable and selecting a few features that seem relevant
features = ['Priority', 'Subcategory ID', 'Organization ID']
target = 'Status'

# Preprocessing
# Dropping rows with missing values in the features or target for simplicity
data_clean = df.dropna(subset=features + [target]) # Ensure to replace 'data' with your actual DataFrame variable name if different

# Encoding categorical variables if needed
# In this case, the features selected are numerical so direct encoding is not necessary

X = data_clean[features]
y = data_clean[target]

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Optional: Feature Scaling
# scaler = StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# Model Training
# Using Logistic Regression
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train) # Replace with X_train_scaled if using scaling

# Predictions
predictions = model.predict(X_test) # Replace with X_test_scaled if using scaling

# Calculating the accuracy as a measure of test error rate
accuracy = accuracy_score(y_test, predictions)
print(accuracy)

from sklearn.metrics import mean_squared_error
import numpy as np

# Assuming 'y_test' contains the actual class labels
# Predicting probabilities for the positive class (assuming it's labeled as '1')
prob_predictions = model.predict_proba(X_test)[:, 1] # Replace with X_test_scaled if using scaling

# Since MSE for classification isn't typical, this treats the problem as if predicting the probability of class 1 accurately is the goal
mse = mean_squared_error(y_test, prob_predictions)

print(f"Mean Squared Error: {mse}")

# Interpreting the MSE
# For classification, especially binary, MSE can range from 0 to 1 (perfectly wrong prediction)
# Lower MSE indicates better fit, as it means the predicted probabilities are closer to the actual class labels

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Since the necessary data preprocessing steps have been outlined but not executed in this notebook,
# let's assume the dataset has been appropriately preprocessed according to the given instructions.

# Model Training using Logistic Regression
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Making predictions on the test set
logistic_predictions = logistic_model.predict(X_test)

# Calculating the accuracy as a measure of test error rate
logistic_accuracy = accuracy_score(y_test, logistic_predictions)
logistic_test_error_rate = 1 - logistic_accuracy

logistic_test_error_rate

# Scatter plot of actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, logistic_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line
plt.title('Actual vs. Predicted Status')
plt.xlabel('Actual Status')
plt.ylabel('Predicted Status')
plt.show()

# Since the logistic regression model predicts discrete classes, this plot will help visualize
# how closely the predictions align with the actual values. The diagonal line represents perfect predictions.