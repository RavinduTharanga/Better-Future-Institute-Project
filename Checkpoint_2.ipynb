{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Selecting a subset of features for simplicity\n",
        "# Assuming 'Status' as the target variable and selecting a few features that seem relevant\n",
        "features = ['Priority', 'Subcategory ID', 'Organization ID']\n",
        "target = 'Status'\n",
        "\n",
        "# Preprocessing\n",
        "# Dropping rows with missing values in the features or target for simplicity\n",
        "data_clean = df.dropna(subset=features + [target]) # Ensure to replace 'data' with your actual DataFrame variable name if different\n",
        "\n",
        "# Encoding categorical variables if needed\n",
        "# In this case, the features selected are numerical so direct encoding is not necessary\n",
        "\n",
        "X = data_clean[features]\n",
        "y = data_clean[target]\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optional: Feature Scaling\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model Training\n",
        "# Using Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train) # Replace with X_train_scaled if using scaling\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test) # Replace with X_test_scaled if using scaling\n",
        "\n",
        "# Calculating the accuracy as a measure of test error rate\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "id": "f3Kn1LEULZjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'y_test' contains the actual class labels\n",
        "# Predicting probabilities for the positive class (assuming it's labeled as '1')\n",
        "prob_predictions = model.predict_proba(X_test)[:, 1] # Replace with X_test_scaled if using scaling\n",
        "\n",
        "# Since MSE for classification isn't typical, this treats the problem as if predicting the probability of class 1 accurately is the goal\n",
        "mse = mean_squared_error(y_test, prob_predictions)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Interpreting the MSE\n",
        "# For classification, especially binary, MSE can range from 0 to 1 (perfectly wrong prediction)\n",
        "# Lower MSE indicates better fit, as it means the predicted probabilities are closer to the actual class labels\n"
      ],
      "metadata": {
        "id": "CIGaVtBlMGmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Since the necessary data preprocessing steps have been outlined but not executed in this notebook,\n",
        "# let's assume the dataset has been appropriately preprocessed according to the given instructions.\n",
        "\n",
        "# Model Training using Logistic Regression\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "logistic_predictions = logistic_model.predict(X_test)\n",
        "\n",
        "# Calculating the accuracy as a measure of test error rate\n",
        "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
        "logistic_test_error_rate = 1 - logistic_accuracy\n",
        "\n",
        "logistic_test_error_rate\n"
      ],
      "metadata": {
        "id": "i6CLnCwpOPA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of actual vs. predicted values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, logistic_predictions, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line\n",
        "plt.title('Actual vs. Predicted Status')\n",
        "plt.xlabel('Actual Status')\n",
        "plt.ylabel('Predicted Status')\n",
        "plt.show()\n",
        "\n",
        "# Since the logistic regression model predicts discrete classes, this plot will help visualize\n",
        "# how closely the predictions align with the actual values. The diagonal line represents perfect predictions.\n"
      ],
      "metadata": {
        "id": "X0b6wpnoRIsm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}